## What Does a Deep Learning Automated Essay Scoring System Know About Good Essays
Course project from [CS 396/496 Deep Learning](https://interactiveaudiolab.github.io/teaching/deeplearning.html#top) taught by Professor [Bryan Pardo](https://users.cs.northwestern.edu/~pardo/) at Northwestern University, Fall 2020.


### Team members:

- Nicole Hessler ([nicolehessler2021@u.northwestern.edu](nicolehessler2021@u.northwestern.edu))
- Yihong Hu ([yihonghu2021@u.northwestern.edu](yihonghu2021@u.northwestern.edu))
- Evey Huang ([eveyhuang@u.northwestern.edu](eveyhuang@u.northwestern.edu))


### Project Overview
Essay writing has been a key part of the student assessment process in standardized exams such as the SAT and GRE. Millions of students across the world take such exams each year, resulting in a huge burden for human graders to grade such a high volume of writing efficiently and consistently. With recent advancement in neural networks and natural language processing, there is a possibility to scale up human graders’ ability and reduce the amount of time needed to grade large numbers of essays in standardized tests and eliminate human bias caused by raters’ expertise and inconsistency.

In our project, we replicated a neural model for automated essay scoring [(Taghipour & Ng, 2016)](https://www.aclweb.org/anthology/D16-1193.pdf), using the [Automated Student Assessment Prize (ASAP) dataset](https://www.kaggle.com/c/asap-aes) released on Kaggle, evaluated with 5-fold cross validation, and tested how the model performs on data it has not seen before.

Our network contains a lookup table, an optional convolution layer, a recurrent layer with LSTM units, and finally a fully connected layer with a sigmoid activation. For evaluation, we used the quadratic weighted kappa (QWK) as the metric. This metric measures the correlation of agreements between human raters and the model and is adopted as the official metric by ASAP. A QWK score closer to 1 means a higher agreements between the human and machine (indicating that the model is producing scores similar to those of human raters). We then ran 5-fold evaluation to measure the performance of the model. The result is shown in the two graphs below (one for loss, and one for QWK).

Loss          |  QWK
:-------------------------:|:-------------------------:
![img1](val_loss1.svg)  |  ![img2](val_qwk1.svg)


To test whether our model is actually learning anything about good writing, we tested it with (a) [an Essay published on New Yorker](https://www.newyorker.com/business/currency/are-computers-making-society-more-unequal ), (b) an essay generated by Babel, (c) a random list of words. We broke down each essays into length of 100, 200 ... to 1000 and see how the scores will vary by length. The result of the experiment is shown in the graph below.

![img3](grades_by_length.png)

While our system performs competitively as promised in the Taghipour and Ng paper, we fail to conclude that it has learned what makes a good English essay. We should expect the professionally written **New Yorker** essay to perform better than the on-topic generated essay, which should perform better than the off-topic generated essay, which should perform better than the generated list of random words. However, the essays' scores are more clearly determined by their length than their substance. Compared to other AES approaches with heavy feature engineering (such as coherence, clarity, or grammar), our approach of directly feeding in the tokenized text to the model resulted in a model that evaluates essays as merely lists of words. We conducted a one-sided t-test on each pair of essays and found that none of the differences between essays is statistically significant. This indicates that the system is unable to discern the difference in quality between a collection of unrelated words, a syntactically correct essay, and an overall coherent essay.

### In case you are interested in knowing more...
You can check out our final paper [here](final_paper_HHH.pdf)
